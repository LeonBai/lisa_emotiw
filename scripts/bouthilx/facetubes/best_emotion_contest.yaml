!obj:pylearn2.train.Train {

    dataset: &train !obj:contestTransformerDataset.TransformerDataset {
        raw: &raw !obj:emotiw.bouthilx.datasets.AFEWDataset {
             base_path: '/data/lisatmp/bouthilx/',
             which_set: 'train',
             preprocessor : &prepro !obj:pylearn2.datasets.preprocessing.Standardize {},
             fit_preprocessor: True,
             fit_test_preprocessor: True,
        },
        transformer : !obj:transformer.TransformationPipeline {
            input_space: !obj:pylearn2.space.Conv2DSpace {
                shape: [96, 96],
                num_channels: 3,
            },
            transformations: [

                !obj:transformer.Flipping { p: 0.2 },

                # If the new image is smaller, the empty space is black.
                !obj:transformer.Scaling {
                     p: 0.9,
                     fct_settings: {
                         loc: 1.0,
                         scale: 0.15
                     },
                },

                !obj:transformer.Rotation {
                    p: 0.8,
                     fct_settings: {
                        loc: 0.0,
                        scale: 15  
                     }
                },
                !obj:transformer.Translation {
                    p: 1.0,
                    fct_settings: {
                        loc: 0.0,
                        scale: 3  
                     }
                },

                !obj:transformer.Occlusion { 
                    p: 0.6,
                    nb: 20,
                    fct_settings: {
                        low: 2.0,  
                        high: 8.0
                    },
                },
 
             ] },
        space_preserving : True,
    },

    model: !obj:pylearn2.models.mlp.MLP {
        layers : [

            !obj:pylearn2.models.mlp.ConvRectifiedLinear {
                     layer_name: 'h0',
                     output_channels: 64,
                     irange: .05,
                     kernel_shape: [5, 5],
                     pool_shape: [4, 4],
                     pool_stride: [2, 2],
                     max_kernel_norm: 1.9365,
                     border_mode: 'full'
            }, !obj:pylearn2.models.mlp.ConvRectifiedLinear {
                     layer_name: 'h1',
                     output_channels: 64,
                     irange: .05,
                     kernel_shape: [5, 5],
                     pool_shape: [4, 4],
                     pool_stride: [2, 2],
                     max_kernel_norm: 1.9365,
                     border_mode: 'full'
            }, !obj:pylearn2.models.mlp.ConvRectifiedLinear {
                     layer_name: 'h2',
                     output_channels: 64,
                     irange: .05,
                     kernel_shape: [5, 5],
                     pool_shape: [4, 4],
                     pool_stride: [3, 3],
                     max_kernel_norm: 1.9365,
                     border_mode: 'full'
            }, !obj:pylearn2.models.mlp.RectifiedLinear {
                     layer_name: 'h3',
                     dim: 1024,
                     sparse_init: 15,
            }, !obj:pylearn2.models.mlp.RectifiedLinear {
                     layer_name: 'h4',
                     dim: 64,
                     sparse_init: 15,
            }, !obj:pylearn2.models.mlp.Softmax {
                layer_name: 'y',
                init_bias_target_marginals: *train,
                irange: .0,
                n_classes: 7
            }
        ],

        input_space: !obj:pylearn2.space.Conv2DSpace {
            shape: [96, 96],
            num_channels: 3
        }

    },

    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        batch_size: 64,
        learning_rate: .1,
#        init_momentum: .5,
        monitoring_dataset:
            {
                'trainsub' : !obj:emotiw.bouthilx.datasets.AFEWDataset {
                    base_path: '/home/xavier/data/icml_2013_emotions',
                    which_set: 'train',
                    start: 0,
                    stop: 2000,
                    preprocessor: *prepro,
                    fit_preprocessor: True
                },

                'valid' : !obj:emotiw.bouthilx.datasets.AFEWDataset {
                    which_set: 'valid',
                    start: 0,
                    stop: 2000,
                    preprocessor: *prepro
                }
            },
            
#        cost: !obj:pylearn2.costs.cost.SumOfCosts { costs: [
#            !obj:pylearn2.costs.cost.MethodCost {
#                method: 'cost_from_X',
#                supervised: 1
#            }, !obj:pylearn2.costs.mlp.dropout.Dropout {
#                 default_input_include_prob: 0.8,
#                 default_input_scale: 1.25
#            }],
#        },
 
        termination_criterion: !obj:pylearn2.termination_criteria.MonitorBased {
            channel_name: "valid_y_misclass",
            prop_decrease: 0.,
            N: 100
        },
    },

    extensions: [
        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
             channel_name: 'valid_y_misclass',
             save_path: "${PYLEARN2_TRAIN_FILE_FULL_STEM}_best.pkl"
        }, !obj:pylearn2.training_algorithms.sgd.OneOverEpoch {
                start: 15,
        }
    ],
}
